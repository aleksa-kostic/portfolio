{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ac0409",
   "metadata": {},
   "source": [
    "# Code for constructing the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567dbd96",
   "metadata": {},
   "source": [
    "```index_constructor.py```\n",
    "\n",
    "```python\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlalchemy\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "        Converts treebank tags to Wordnet POS names.\n",
    "\n",
    "        Whole function (modified): Suzana\n",
    "        https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "\n",
    "        RESOURCE\n",
    "        All possible tags : https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "    \"\"\"\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'  # treat token as noun.\n",
    "\n",
    "\n",
    "def removeStopwords(word_list):\n",
    "    \"\"\"\n",
    "        Removes English stopwords from a list of words.\n",
    "        List comprehension done by : Daren Thomas\n",
    "        https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "    \"\"\"\n",
    "\n",
    "    return [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def lemmatize(word_list):\n",
    "    \"\"\"\n",
    "        Lemmatizes a list of two tuples with token and a part of speech pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word, pos=tag) for word, tag in word_list]\n",
    "\n",
    "\n",
    "def clean(word_list):\n",
    "    \"\"\"\n",
    "        Cleans a list of words by removing stop words and applying lemmatization to each word.\n",
    "    \"\"\"\n",
    "    word_list = removeStopwords(word_list)  # Removing stop words\n",
    "    treebank_tags = nltk.pos_tag(word_list)  # Identify parts of speech, need to simplify tags using get_pos_tag()\n",
    "\n",
    "    wordnet_tags = []  # New list with simplified tags, not sure why this takes two steps from NLTK...\n",
    "\n",
    "    for token, tag in treebank_tags:\n",
    "        wordnet_tags.append((token, get_wordnet_pos(tag)))\n",
    "\n",
    "    return lemmatize(wordnet_tags)  # Return lemmatized words\n",
    "\n",
    "\n",
    "def getKeywords(soup):\n",
    "    \"\"\"\n",
    "        Gets all keywords (headers, bolded, titles, listed items) from an HTML document, these keywords are important to our search engine as they\n",
    "        carry more weight than the words found in the text.\n",
    "\n",
    "        Returns a list of keywords, empty list if there aren't any.\n",
    "            Could be enhanced by giving weight to each word, e.g. Title tokens have more weight than bolded tokens\n",
    "\n",
    "        Regex found by user phd,\n",
    "        https://stackoverflow.com/questions/45062534/how-to-grab-all-headers-from-a-website-using-beautifulsoup\n",
    "\n",
    "        HTML stripping : Jorge Galvis\n",
    "        https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\n",
    "\n",
    "        Puncuation stripping : rmalouf\n",
    "        https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    raw_keywords = soup.find_all(\n",
    "        [re.compile('^h[1-6]$'), 'b', 'title', 'li'])  # Regex to find all keywords, see citation above.\n",
    "\n",
    "    keywords = []\n",
    "\n",
    "    # Stripping keyword tags\n",
    "    for raw_keyword in raw_keywords:\n",
    "        keyword = raw_keyword.get_text(\" \", strip=True).lower().replace('\\n', \" \")\n",
    "\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(\n",
    "            r'\\w+\\'?\\w*')  # Getting rid of unnecessary punctuation (preserving apostrophes)\n",
    "        keywords.extend(tokenizer.tokenize(keyword))\n",
    "\n",
    "    return clean(keywords)\n",
    "\n",
    "\n",
    "def getTokens(soup):\n",
    "    \"\"\"\n",
    "        Gets all tokens from html, removes stop words and lemmatizes. Returns list of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    text = soup.get_text(\" \", strip=True).lower().replace('\\n', \" \")\n",
    "\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(\n",
    "        r'\\w+\\'?\\w*')  # Getting rid of unnecessary punctuation (preserving apostrophes)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    return clean(tokens)\n",
    "\n",
    "\n",
    "def mult_dir(dir):\n",
    "    root_directory = 'C:/Users/aKost/Desktop/2021-2022/WINTER 2022/CS 121 - Information Retrieval/project3/WEBPAGES_RAW'\n",
    "    engine = sqlalchemy.create_engine('postgresql+psycopg2://postgres:qrT90!xvnpc@localhost/cs121-p3')\n",
    "    for file in tqdm(os.listdir(os.path.join(root_directory, dir))):  # Iterating over files\n",
    "        with open(os.path.join(root_directory, dir, file), 'r', encoding=\"utf8\") as f:  # Opening file\n",
    "            f = f.read()\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "            path = os.path.join(root_directory, dir, file)\n",
    "\n",
    "            keywords = getKeywords(soup)  # Getting keywords\n",
    "            tokens = getTokens(soup)  # Getting all text tokens\n",
    "\n",
    "            insertTokens(tokens, path, dir, file, engine)\n",
    "            insertTokens(keywords, path, dir, file, engine, isKeyword=True)\n",
    "\n",
    "def constructIndex(root_directory):\n",
    "    \"\"\"\n",
    "        Gets all the html in the specified path to extract tokens and stores them in a Postgres server.\n",
    "    \"\"\"\n",
    "    engine = sqlalchemy.create_engine('postgresql+psycopg2://postgres:qrT90!xvnpc@localhost/cs121-p3')\n",
    "\n",
    "    for subdir, dirs, files in os.walk(root_directory):  # Iterating over folders in WEBPAGES_RAW_TEST\n",
    "        a_pool = multiprocessing.Pool()\n",
    "        a_pool.map(mult_dir, dirs)\n",
    "\n",
    "    # engine.execute(\"CREATE INDEX inv_idx ON Tokens USING gin(token)\")\n",
    "\n",
    "\n",
    "def insertTokens(tokens, full_path, dir, file, engine, isKeyword=False, frequency=1):\n",
    "    for token in tokens:\n",
    "\n",
    "        token_info = (\n",
    "            token,\n",
    "            isKeyword,  # isKeyword\n",
    "            full_path,\n",
    "            dir,\n",
    "            file,\n",
    "            frequency\n",
    "        )\n",
    "\n",
    "        engine.execute(\n",
    "            \"INSERT INTO Tokens VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (token, fullPath) DO UPDATE SET token = EXCLUDED.token, isKeyword = EXCLUDED.isKeyword, fullPath = EXCLUDED.fullPath, dir = EXCLUDED.dir, file = EXCLUDED.file, frequency = EXCLUDED.frequency + 1;\",\n",
    "            token_info)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    rootdir = 'C:/Users/aKost/Desktop/2021-2022/WINTER 2022/CS 121 - Information Retrieval/project3/WEBPAGES_RAW'\n",
    "\n",
    "    constructIndex(rootdir)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1dc80",
   "metadata": {},
   "source": [
    "# SQL setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635a804",
   "metadata": {},
   "source": [
    "```setup.sql```\n",
    "\n",
    "```sql\n",
    "SET CLIENT_ENCODING = \"utf8\";\n",
    "\n",
    "DROP TABLE IF EXISTS Tokens;\n",
    "CREATE TABLE IF NOT EXISTS Tokens\n",
    "(\n",
    "    token         TEXT,\n",
    "    isKeyword    BOOLEAN,\n",
    "    fullPath    TEXT,\n",
    "    dir            VARCHAR(5),\n",
    "    file        VARCHAR(5),\n",
    "    frequency    INTEGER,\n",
    "    PRIMARY KEY (Token, fullPath)\n",
    ");\n",
    "\n",
    "CREATE EXTENSION IF NOT EXISTS pg_trgm ;\n",
    "CREATE INDEX IF NOT EXISTS tkn_trgm_idx ON Tokens USING gin(token gin_trgm_ops);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f435a3",
   "metadata": {},
   "source": [
    "# SQL Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f45775",
   "metadata": {},
   "source": [
    "```queries.sql```\n",
    "\n",
    "```sql\n",
    "\n",
    "-- NUMBER OF DOCUMENTS\n",
    "SELECT COUNT(DISTINCT t.fullPath)\n",
    "FROM Tokens t;\n",
    "\n",
    "-- NUMBER OF UNIQUE WORDS\n",
    "\n",
    "SELECT COUNT(DISTINCT t.token)\n",
    "FROM Tokens t;\n",
    "\n",
    "--- INDEX SIZE ---\n",
    "select pg_indexes_size('Tokens');\n",
    "\n",
    "-- QUERIES --\n",
    "\n",
    "-- \"Informatics\" query\n",
    "\n",
    "SELECT *\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'informatics'\n",
    "ORDER BY t.Frequency DESC\n",
    "LIMIT 20;\n",
    "\n",
    "-- COUNT\n",
    "SELECT COUNT(t.token)\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'informatics'\n",
    "GROUP BY t.token;\n",
    "\n",
    "\n",
    "-- \"Mondego\" query\n",
    "\n",
    "SELECT *\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'mondego'\n",
    "ORDER BY t.Frequency DESC\n",
    "LIMIT 20;\n",
    "\n",
    "\n",
    "-- COUNT\n",
    "SELECT COUNT(t.token)\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'mondego'\n",
    "GROUP BY t.token;\n",
    "\n",
    "\n",
    "-- \"Irvine\" query\n",
    "\n",
    "SELECT *\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'irvine'\n",
    "ORDER BY t.Frequency DESC\n",
    "LIMIT 20;\n",
    "\n",
    "\t-- COUNT\n",
    "SELECT COUNT(t.token)\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'irvine'\n",
    "GROUP BY t.token;\n",
    "\n",
    "\n",
    "-- \"artificial intelligence\" query\n",
    "\n",
    "SELECT *\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'artificial' OR t.token = 'intelligence'\n",
    "ORDER BY t.frequency DESC\n",
    "LIMIT 20;\n",
    "\n",
    "\n",
    "-- COUNT\n",
    "SELECT COUNT(t.token)\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'artificial' OR t.token = 'intelligence'\n",
    "GROUP BY t.token;\n",
    "\n",
    "\n",
    "-- \"computer science\" query\n",
    "\n",
    "SELECT *\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'computer' OR t.token = 'science'\n",
    "ORDER BY t.frequency DESC\n",
    "LIMIT 20;\n",
    "\n",
    "\n",
    "-- COUNT\n",
    "SELECT COUNT(t.token)\n",
    "FROM Tokens t\n",
    "WHERE t.token = 'computer' OR t.token = 'science'\n",
    "GROUP BY t.token;\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
